{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"1IufrcOqaFAY","executionInfo":{"status":"ok","timestamp":1654755496932,"user_tz":-330,"elapsed":538,"user":{"displayName":"Arti Gupta","userId":"17690262578366801034"}}},"outputs":[],"source":["from gym import Env\n","from gym.spaces import Discrete, Box\n","import numpy as np\n","import random"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"nZVWf5VRaFAa","executionInfo":{"status":"ok","timestamp":1654755496933,"user_tz":-330,"elapsed":4,"user":{"displayName":"Arti Gupta","userId":"17690262578366801034"}}},"outputs":[],"source":["baseline = []  #packets sent in baseline\n","drl = []       #packets sent in DRL  \n","power_baseline = []\n","power_drl = []\n","Network_energy_drl=[]\n","Network_energy_baseline=[]"]},{"cell_type":"markdown","metadata":{"id":"R-_eNHEO7Iy2"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"NQL-0Oxz40pW"},"source":[""]},{"cell_type":"code","execution_count":3,"metadata":{"id":"ajra87t-aFAb","executionInfo":{"status":"ok","timestamp":1654755496933,"user_tz":-330,"elapsed":4,"user":{"displayName":"Arti Gupta","userId":"17690262578366801034"}}},"outputs":[],"source":["# import matplotlib.pyplot as plt\n","# import numpy as np\n","\n","# X = np.array([i for i in range(1000)])\n","# y = np.array(baseline_power)\n","# z = np.array(drl_power)\n","\n","\n","# plt.plot(X, y, color='r', label='Baseline model')\n","# plt.plot(X, z, color='g', label='DRL based model')\n","  \n","# # Naming the x-axis, y-axis and the whole graph\n","\n","# plt.xlabel(\"Episodes\")\n","# plt.ylabel(\"Power Available\")\n","# plt.title(\"Battery Drain w.r.to Baseline and DRL\")\n","  \n","# # Adding legend, which helps us recognize the curve according to it's color\n","# plt.legend()\n","# plt.xlim([0, 1200]) \n","# plt.ylim([0, 18000]) \n","# plt.rcParams[\"figure.figsize\"] = (14,7)\n","# # To load the display window\n","# plt.show()"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"8xOzRKBYaFAc","executionInfo":{"status":"ok","timestamp":1654755497813,"user_tz":-330,"elapsed":883,"user":{"displayName":"Arti Gupta","userId":"17690262578366801034"}}},"outputs":[],"source":["class Wireless_Env_Baseline(Env):\n","    def __init__(self):\n","        \n","        #Packet Generation Rates calculated from Auto-Regression Model\n","        self.pgr = [1, 1, 1, 1, 1, 1, 1, 1]\n","        \n","        #These are packet queues for each sensor\n","        self.pulse = []\n","        self.heart_rate = []\n","        self.temperature = []\n","        self.diabetes = []\n","        self.spo2 = []\n","        self.ibi = []\n","        self.eda = []\n","        self.acc = []\n","      \n","        \n","        #Total initial power\n","        self.power = 4        \n","        \n","        \n","        #Step count for each episode. It can be changed as per requirement.\n","        self.count = 1001\n","        \n","        #self.sensors = {0 : \"Pulse\", 1 : \"Heart Rate\", 2 : \"Temprature\", 3 : \"Diabetes\",  : \"SpO2\", 5 : \"EDA\", 6 : \"IBI\", 7 : \"ACC\"}\n","                \n","        # Actions we can take ---> we can select any of 8 sensors for packet transmission.\n","        self.action_space = Discrete(8)\n","        \n","        \n","        #initial timestamp (Optional---> I have used it for testing purpose.)\n","        self.time_stamp = 1\n","        \n","        #Set the network length(Will update the battery power,,, We can update here the minimum battery sensor)\n","        self.life = 1000        \n","        \n","        #Initial number of packets in queues\n","        self.state = 0        \n","        \n","        \n","        #This is observation space..will take any random value( Just for model shake...No use in our case.)\n","        self.observation_space = Box(low=np.array([0]), high=np.array([100]))\n","    \n","    \n","    #calculating reward based on our reward function.\n","    def get_reward(self, packet_gen_prob, self_queue, others_queue):\n","        #print(\"In Reward\")\n","        reward_ = ((len(self_queue) * packet_gen_prob) + 1) / (others_queue - len(self_queue) + 1) + .25\n","        return reward_\n","     \n","        \n","    #This is crucial function. It defines how the state is changing after each selection. Every update is happening here.    \n","    def state_change(self): \n","        \n","        #In baseline...adding 1 packet into each sensor queue as each sensor is sensing 1 packet per step.\n","        self.ibi.append(1)\n","        self.eda.append(1)        \n","        self.diabetes.append(1)        \n","        self.heart_rate.append(1)\n","        self.acc.append(1)       \n","        self.temperature.append(1)        \n","        self.pulse.append(1)       \n","        self.spo2.append(1)\n","            \n","    def step(self, action):       \n","        \n","        #for testing.\n","        self.count -= 1\n","        \n","        #Power equation used here. Can be changed as per requirement.\n","        self.power -= (16.31 * 10e-9 * 256 + 1.97 * 10e-9 * 100 * 256)\n","        \n","        #Testing\n","        #print(\"Selected sensor is --> \", action)\n","        \n","        #Changing the state.\n","        self.state_change()\n","        \n","        total_pending_packets = len(self.pulse) + len(self.heart_rate) + len(self.temperature) + len(self.diabetes) + len(self.spo2) + len(self.ibi) + len(self.eda) + len(self.acc) \n","        \n","        #print(\"total pending packets -----> \", total_pending_packets)\n","        \n","        \n","        #In this if else ladder, we are updating based on selected node.\n","        \n","        if(self.count > 0):\n","            baseline.append(total_pending_packets)\n","#            \n","        if(self.power > 0):\n","            power_baseline.append(self.power)\n","        if(self.power <= 0):\n","            power_baseline.append(0)\n","        Network_energy_baseline.append(self.power)\n","\n","\n","\n","        if action == 0:\n","            #print(\" Sensor Queue size is \", len(self.pulse))\n","            if(len(self.pulse) == 0):\n","                reward = -1\n","            else:\n","                \n","                reward = self.get_reward(self.pgr[0], self.pulse, total_pending_packets)\n","                if(len(self.pulse) != 0):\n","                    self.pulse.pop()\n","        \n","        elif action == 1:\n","            #print(\" Sensor Queue size is \", len(self.heart_rate))\n","            if(len(self.heart_rate) == 0):\n","                reward = -1\n","            else:\n","                \n","                reward = self.get_reward(self.pgr[1], self.heart_rate, total_pending_packets)\n","                if len(self.heart_rate) != 0:\n","                    self.heart_rate.pop()\n","        \n","        elif action == 2:\n","            #print(\" Sensor Queue size is \", len(self.temperature))\n","            if len(self.temperature) == 0:\n","                reward = -1\n","            else:\n","                \n","                reward = self.get_reward(self.pgr[2], self.temperature, total_pending_packets)        \n","                if len(self.temperature) != 0:\n","                    self.temperature.pop()\n","            \n","        elif action == 3:\n","            #print(\" Sensor Queue size is \", len(self.diabetes ) )\n","            if(len(self.diabetes) == 0):\n","                reward = -1\n","            else:\n","                \n","                reward = self.get_reward(self.pgr[3], self.diabetes, total_pending_packets)\n","                if len(self.diabetes) != 0:\n","                    self.diabetes.pop()\n","        \n","        elif action == 4:\n","            #print(\" Sensor Queue size is \", len(self.spo2))\n","            if(len(self.spo2) == 0):\n","                reward = -1\n","            else:\n","                \n","                reward = self.get_reward(self.pgr[4], self.spo2, total_pending_packets)        \n","                if len(self.spo2) != 0:\n","                    self.spo2.pop()\n","        elif action == 5:\n","            #print(\" Sensor Queue size is \",len(self.eda))\n","            if(len(self.eda) == 0):\n","                reward = -1\n","            else:\n","                \n","                reward = self.get_reward(self.pgr[5], self.eda, total_pending_packets)        \n","                if len(self.eda) != 0:\n","                    self.eda.pop()\n","        elif action == 6:\n","            #print(\" Sensor Queue size is \", len(self.ibi))\n","            if(len(self.ibi) == 0):\n","                reward = -1\n","            else:\n","                \n","                reward = self.get_reward(self.pgr[6], self.ibi, total_pending_packets)        \n","                if len(self.ibi) != 0:\n","                    self.ibi.pop()\n","        else:\n","            #print(\" Sensor Queue size is \",len(self.acc))\n","            if(len(self.acc) == 0):\n","                reward = -1\n","            else:\n","                \n","                reward = self.get_reward(self.pgr[7], self.acc, total_pending_packets)        \n","                if len(self.acc) != 0:\n","                    self.acc.pop()  \n","        \n","        \n","        #Reducing life by 1, here we can also use battery power. \n","        self.life -= 1    \n","        \n","       \n","        \n","        if self.life <= 0: \n","            done = True\n","        else:\n","            done = False   \n","        \n","        # Apply temperature noise\n","        #self.state += random.randint(-1,1)\n","        \n","        # Set placeholder for info\n","        info = {}\n","        \n","        # Return step information\n","        return self.state, reward, done, info\n","\n","    def render(self):\n","        #For Visualization Purpose\n","        pass\n","    \n","    def reset(self):\n","        # Reset shower temperature\n","        self.state = 0\n","        # Reset shower time\n","        self.life = 1000\n","        \n","        #Reset all packet queues\n","        #print('=---------p--------------',self.pulse)\n","        self.pulse = []\n","        self.heart_rate = []\n","        self.temperature = []\n","        self.diabetes = []\n","        self.spo2 = []\n","        self.ibi = []\n","        self.eda = []\n","        self.acc = []\n","        \n","        return self.state\n","    \n","    "]},{"cell_type":"code","execution_count":4,"metadata":{"id":"Ryb9w6_dRzPe","executionInfo":{"status":"ok","timestamp":1654755497813,"user_tz":-330,"elapsed":17,"user":{"displayName":"Arti Gupta","userId":"17690262578366801034"}}},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":5,"metadata":{"id":"enx6kP9jaFAf","executionInfo":{"status":"ok","timestamp":1654755497814,"user_tz":-330,"elapsed":17,"user":{"displayName":"Arti Gupta","userId":"17690262578366801034"}}},"outputs":[],"source":["\n","#Here also, all parameters are same as Baseline Model apart from PGR.\n","class Wireless_Env_DRL(Env):\n","    def __init__(self):\n","        \n","        #Packet Generation Rates calculated from Auto-Regression Model\n","        self.pgr = [0.09, 0.162, 0.005, 0.712, 0.03, 0.976, 0.941, 0.19]\n","        \n","        self.pulse = []\n","        self.heart_rate = []\n","        self.temperature = []\n","        self.diabetes = []\n","        self.spo2 = []\n","        self.ibi = []\n","        self.eda = []\n","        self.acc = []\n","        \n","        self.power = [.5, .5, .5, .5, .5, .5, .5, .5]\n","        self.count = 1001\n","        #self.flag = 1\n","        #self.sensors = {0 : \"Pulse\", 1 : \"Heart Rate\", 2 : \"Temprature\", 3 : \"Diabetes\",  : \"SpO2\", 5 : \"EDA\", 6 : \"IBI\", 7 : \"ACC\"}\n","                \n","        # Actions we can take, down, stay, up\n","        self.action_space = Discrete(8)\n","        \n","        self.time_stamp = 1\n","        \n","        #Set the network length(Will update the battery power,,, We can update here the minimum battery sensor)\n","        self.life = 1000       \n","        \n","        #Initial number of packets in queues\n","        self.state = 0        \n","        \n","        self.observation_space = Box(low=np.array([0]), high=np.array([100]))\n","        \n","    def get_reward(self, packet_gen_prob, self_queue, others_queue):\n","        #print(\"In Reward\")\n","        reward_ = (((len(self_queue) * packet_gen_prob) + 1) / (others_queue - len(self_queue) + 1) + .25)\n","        return reward_\n","        \n","    def state_change(self): \n","        \n","        ts = self.time_stamp\n","        \n","        self.time_stamp += 1\n","        \n","        self.ibi.append(1)\n","        self.eda.append(1)        \n","        self.diabetes.append(1)        \n","        \n","        if ts % 2 == 0:\n","            self.heart_rate.append(1)\n","        \n","        if ts % 2 == 0:\n","            self.acc.append(1)\n","        \n","        if ts % 3 == 0:\n","            self.temperature.append(1)\n","            \n","        if ts % 11 == 0:\n","            self.pulse.append(1)            \n","            \n","        if ts % 30 == 0:\n","            self.spo2.append(1)\n","            \n","        \n","    \n","    def step(self, action):       \n","        \n","        self.count -= 1\n","        \n","        #Power function. But we will reduce power only when sensor is selected.\n","        p = (16.31 * 10e-9 * 256 + 1.97 * 10e-9 * 100 * 256)\n","        \n","        \n","        #print(\"Selected sensor is --> \", action)\n","        \n","        self.state_change()\n","        \n","        total_pending_packets = len(self.pulse) + len(self.heart_rate) + len(self.temperature) + len(self.diabetes) + len(self.spo2) + len(self.ibi) + len(self.eda) + len(self.acc) \n","        \n","        #print(\"total pending packets -----> \", total_pending_packets)\n","        \n","        if(self.count > 0):\n","            \n","            drl.append(total_pending_packets)\n","            \n","        if(sum(self.power) > 0):\n","            power_drl.append(sum(self.power))\n","        \n","        Network_energy_drl.extend(self.power)\n","        if action == 0:\n","            #print(\" Sensor Queue size is \", len(self.pulse))\n","            if(len(self.pulse) == 0):\n","                reward = -1\n","            else:\n","                if(self.power[0] - p > 0): #here I am reducing power if this sensor is selected. same for others as well.\n","                    self.power[0] -= p\n","                    #print(power)\n","                reward = self.get_reward(self.pgr[0], self.pulse, total_pending_packets)\n","                if(len(self.pulse) != 0):\n","                    self.pulse.pop()\n","        \n","        elif action == 1:\n","            #print(\" Sensor Queue size is \", len(self.heart_rate))\n","            if(len(self.heart_rate) == 0):\n","                reward = -1\n","            else:\n","                if(self.power[1] - p > 0):\n","                    self.power[1] -= p\n","                reward = self.get_reward(self.pgr[1], self.heart_rate, total_pending_packets)\n","                if len(self.heart_rate) != 0:\n","                    self.heart_rate.pop()\n","        \n","        elif action == 2:\n","            #print(\" Sensor Queue size is \", len(self.temperature))\n","            if len(self.temperature) == 0:\n","                reward = -1\n","            else:\n","                if(self.power[2] - p > 0):\n","                    self.power[2] -= p\n","                reward = self.get_reward(self.pgr[2], self.temperature, total_pending_packets)        \n","                if len(self.temperature) != 0:\n","                    self.temperature.pop()\n","            \n","        elif action == 3:\n","            #print(\" Sensor Queue size is \", len(self.diabetes ) )\n","            if(len(self.diabetes) == 0):\n","                reward = -1\n","            else:\n","                if(self.power[3] - p > 0):\n","                    self.power[3] -= p\n","                reward = self.get_reward(self.pgr[3], self.diabetes, total_pending_packets)\n","                if len(self.diabetes) != 0:\n","                    self.diabetes.pop()\n","        \n","        elif action == 4:\n","            #print(\" Sensor Queue size is \", len(self.spo2))\n","            if(len(self.spo2) == 0):\n","                reward = -1\n","            else:\n","                if(self.power[4] - p > 0):\n","                    self.power[4] -= p\n","                reward = self.get_reward(self.pgr[4], self.spo2, total_pending_packets)        \n","                if len(self.spo2) != 0:\n","                    self.spo2.pop()\n","        elif action == 5:\n","            #print(\" Sensor Queue size is \",len(self.eda))\n","            if(len(self.eda) == 0):\n","                reward = -1\n","            else:\n","                if(self.power[5] - p > 0):\n","                    self.power[5] -= p\n","                reward = self.get_reward(self.pgr[5], self.eda, total_pending_packets)        \n","                if len(self.eda) != 0:\n","                    self.eda.pop()\n","        elif action == 6:\n","            #print(\" Sensor Queue size is \", len(self.ibi))\n","            if(len(self.ibi) == 0):\n","                reward = -1\n","            else:\n","                if(self.power[6] - p > 0):\n","                    self.power[6] -= p\n","                reward = self.get_reward(self.pgr[6], self.ibi, total_pending_packets)        \n","                if len(self.ibi) != 0:\n","                    self.ibi.pop()\n","        else:\n","            #print(\" Sensor Queue size is \",len(self.acc))\n","            if(len(self.acc) == 0):\n","                reward = -1\n","            else:\n","                if(self.power[7] - p > 0):\n","                    self.power[7] -= p\n","                reward = self.get_reward(self.pgr[7], self.acc, total_pending_packets)        \n","                if len(self.acc) != 0:\n","                    self.acc.pop()  \n","        \n","        \n","        \n","        self.life -= 1    \n","            \n","        if self.life <= 0: \n","            done = True\n","        else:\n","            done = False   \n","        \n","        # Apply temperature noise\n","        #self.state += random.randint(-1,1)\n","        \n","        # Set placeholder for info\n","        info = {}\n","        \n","        # Return step information\n","        return self.state, reward, done, info\n","\n","    def render(self):\n","        #For Visualization Purpose\n","        pass\n","    \n","    #This step resets the parameters after each 1000 steps. For training purpose. can be updated for battery life as well.\n","    def reset(self):\n","        # Reset shower temperature\n","        self.state = 0\n","        # Reset shower time\n","        self.life = 1000\n","        \n","        #Reset all packet queues\n","        self.pulse = []\n","        self.heart_rate = []\n","        self.temperature = []\n","        self.diabetes = []\n","        self.spo2 = []\n","        self.ibi = []\n","        self.eda = []\n","        self.acc = []\n","        \n","        return self.state\n","    \n"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1654755497814,"user":{"displayName":"Arti Gupta","userId":"17690262578366801034"},"user_tz":-330},"id":"CQ3Ue_DkaFAh","outputId":"38c8ba1f-99fc-4a99-ee21-ac1e470ebe7b"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n","  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"]}],"source":["env_drl = Wireless_Env_DRL() "]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1654755497815,"user":{"displayName":"Arti Gupta","userId":"17690262578366801034"},"user_tz":-330},"id":"3COZZ3iqaFAj","outputId":"ee380f17-fc65-4b49-d09a-bc4a220cbd33"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n","  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"]}],"source":["env_bsl = Wireless_Env_Baseline()"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1654755497815,"user":{"displayName":"Arti Gupta","userId":"17690262578366801034"},"user_tz":-330},"id":"hs64JwcJaFAk","outputId":"5c4887f7-877d-4ef0-90a1-ab6d2c4d0077"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([10.242901], dtype=float32)"]},"metadata":{},"execution_count":8}],"source":["env_bsl.observation_space.sample()"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1654755497815,"user":{"displayName":"Arti Gupta","userId":"17690262578366801034"},"user_tz":-330},"id":"GX3gBMDNaFAk","outputId":"5b5e4985-8af9-47e4-8470-115512616329"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([82.37394], dtype=float32)"]},"metadata":{},"execution_count":9}],"source":["env_drl.observation_space.sample()"]},{"cell_type":"code","source":[" episodes = 10\n"," sco_drl=[]\n"," for episode in range(1, episodes+1):\n","     state = env_drl.reset()\n","     done = False\n","     score = 0 \n","    \n","     while not done:\n","         #env.render()\n","         action = env_drl.action_space.sample()\n","         n_state, reward, done, info = env_drl.step(action)\n","         score+=reward\n","     print('Episode:{} Reward:{}'.format(episode, score))\n","     sco_drl.append(score)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NRPPkw44x9ka","executionInfo":{"status":"ok","timestamp":1654755498409,"user_tz":-330,"elapsed":601,"user":{"displayName":"Arti Gupta","userId":"17690262578366801034"}},"outputId":"187e2def-15d5-41ae-a947-2e65aeb5325e"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Episode:1 Reward:206.423513217909\n","Episode:2 Reward:190.4318865412673\n","Episode:3 Reward:205.81717624318483\n","Episode:4 Reward:202.55042632337543\n","Episode:5 Reward:162.71223312013785\n","Episode:6 Reward:201.04219752358162\n","Episode:7 Reward:189.29788302172528\n","Episode:8 Reward:230.42845104129378\n","Episode:9 Reward:229.58196406688023\n","Episode:10 Reward:141.06288752520254\n"]}]},{"cell_type":"code","source":["import pandas as pd"],"metadata":{"id":"aKirkBAxI_xy","executionInfo":{"status":"ok","timestamp":1654755498412,"user_tz":-330,"elapsed":15,"user":{"displayName":"Arti Gupta","userId":"17690262578366801034"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s74twJK7JF3r","executionInfo":{"status":"ok","timestamp":1654755519154,"user_tz":-330,"elapsed":20755,"user":{"displayName":"Arti Gupta","userId":"17690262578366801034"}},"outputId":"105b1596-546a-4af7-f25a-086c94165c2e"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["A = np.array(sco_drl)"],"metadata":{"id":"8Qi1l4POJOUh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pd.DataFrame(A).to_csv('/content/drive/MyDrive/Colab Notebooks/reward_drl_e3.csv')"],"metadata":{"id":"lyISLyEzJSI5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[" episodes = 10\n"," sco_bsl=[]\n"," for episode in range(1, episodes+1):\n","     state = env_bsl.reset()\n","     done = False\n","     score = 0 \n","    \n","     while not done:\n","         #env.render()\n","         action = env_bsl.action_space.sample()\n","         n_state, reward, done, info = env_bsl.step(action)\n","         score+=reward\n","     print('Episode:{} Reward:{}'.format(episode, score))\n","     sco_bsl.append(score)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YVp54AW1y_q1","executionInfo":{"status":"ok","timestamp":1654709301807,"user_tz":-330,"elapsed":8,"user":{"displayName":"Arti Gupta","userId":"17690262578366801034"}},"outputId":"69b2e199-cd0e-44a1-cd7e-e94923f191b0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Episode:1 Reward:393.9348778805539\n","Episode:2 Reward:393.99380646678696\n","Episode:3 Reward:394.0458148837243\n","Episode:4 Reward:393.59075480392056\n","Episode:5 Reward:394.013816708271\n","Episode:6 Reward:393.5452841634225\n","Episode:7 Reward:393.4751447276344\n","Episode:8 Reward:393.870777533738\n","Episode:9 Reward:393.95581238845233\n","Episode:10 Reward:393.81894199399534\n"]}]},{"cell_type":"code","source":["B = np.array(sco_bsl)"],"metadata":{"id":"fEaFlAVnPr7W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pd.DataFrame(B).to_csv('/content/drive/MyDrive/Colab Notebooks/reward_bsl_e3.csv')"],"metadata":{"id":"kvL8RvYsPsME"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["B = np.array(sco_bsl)"],"metadata":{"id":"YUUY1nZWKYYF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pd.DataFrame(A).to_csv('/content/drive/MyDrive/Colab Notebooks/reward_bsl_e3.csv')"],"metadata":{"id":"X3vOKoAhKbwE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#sco"],"metadata":{"id":"eJewz-vcXNPE"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":13,"metadata":{"id":"EcliaFAuaFAl","executionInfo":{"status":"ok","timestamp":1654755522931,"user_tz":-330,"elapsed":3782,"user":{"displayName":"Arti Gupta","userId":"17690262578366801034"}}},"outputs":[],"source":["import numpy as np\n","import tensorflow\n","import keras\n","from keras.models import Sequential\n","from keras.layers import Dense, Flatten\n","from tensorflow.keras.optimizers import Adam\n","model = keras.Sequential()"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"-XytEgM2aFAl","executionInfo":{"status":"ok","timestamp":1654755522932,"user_tz":-330,"elapsed":6,"user":{"displayName":"Arti Gupta","userId":"17690262578366801034"}}},"outputs":[],"source":["states = env_bsl.observation_space.shape\n","actions = env_bsl.action_space.n"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"-Om9y8S9aFAm","executionInfo":{"status":"ok","timestamp":1654755522932,"user_tz":-330,"elapsed":5,"user":{"displayName":"Arti Gupta","userId":"17690262578366801034"}}},"outputs":[],"source":["states = env_drl.observation_space.shape\n","actions = env_drl.action_space.n"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":56,"status":"ok","timestamp":1654755523689,"user":{"displayName":"Arti Gupta","userId":"17690262578366801034"},"user_tz":-330},"id":"rdhYDUwaaFAm","outputId":"ad7df935-5084-418a-cdab-c4609a5e6efa"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Box(0.0, 100.0, (1,), float32)"]},"metadata":{},"execution_count":16}],"source":["env_drl.observation_space"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":53,"status":"ok","timestamp":1654755523690,"user":{"displayName":"Arti Gupta","userId":"17690262578366801034"},"user_tz":-330},"id":"EJQqmh9p2DGW","outputId":"cfd39ff3-753b-4ea2-d926-ac6ab5acf800"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Box(0.0, 100.0, (1,), float32)"]},"metadata":{},"execution_count":17}],"source":["env_bsl.observation_space"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":45,"status":"ok","timestamp":1654755523690,"user":{"displayName":"Arti Gupta","userId":"17690262578366801034"},"user_tz":-330},"id":"1Di1yO10aFAm","colab":{"base_uri":"https://localhost:8080/"},"outputId":"da3784a2-1501-4274-ed3f-e708445d3d24"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["8"]},"metadata":{},"execution_count":18}],"source":["actions\n"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"ezRxfEpXoXmH","executionInfo":{"status":"ok","timestamp":1654755550091,"user_tz":-330,"elapsed":629,"user":{"displayName":"Arti Gupta","userId":"17690262578366801034"}}},"outputs":[],"source":["del model"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"tEsCsDPhaFAn","executionInfo":{"status":"ok","timestamp":1654755550750,"user_tz":-330,"elapsed":5,"user":{"displayName":"Arti Gupta","userId":"17690262578366801034"}}},"outputs":[],"source":["def build_model(states, actions):\n","    model = keras.Sequential()\n","    \n","    model.add(Dense(32, activation='relu', input_shape=states))\n","    model.add(Dense(64, activation='relu'))\n","    model.add(Dense(actions, activation='linear'))\n","    return model"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"HQmgcM24aFAn","executionInfo":{"status":"ok","timestamp":1654755550750,"user_tz":-330,"elapsed":3,"user":{"displayName":"Arti Gupta","userId":"17690262578366801034"}}},"outputs":[],"source":["model = build_model(states, actions)"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1654755551333,"user":{"displayName":"Arti Gupta","userId":"17690262578366801034"},"user_tz":-330},"id":"I5ebzUI8aFAo","outputId":"aa21bce7-e7f6-4a2f-b957-cd0592a25316"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_3 (Dense)             (None, 32)                64        \n","                                                                 \n"," dense_4 (Dense)             (None, 64)                2112      \n","                                                                 \n"," dense_5 (Dense)             (None, 8)                 520       \n","                                                                 \n","=================================================================\n","Total params: 2,696\n","Trainable params: 2,696\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["model.summary()"]},{"cell_type":"markdown","source":[""],"metadata":{"id":"G6Af-bsxNOVJ"}},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2962,"status":"ok","timestamp":1654755554291,"user":{"displayName":"Arti Gupta","userId":"17690262578366801034"},"user_tz":-330},"id":"DwpCt_yRbJSJ","outputId":"c6081a95-185b-412d-b178-6869e1412652"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: keras-rl2 in /usr/local/lib/python3.7/dist-packages (1.0.5)\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from keras-rl2) (2.8.2+zzzcolab20220527125636)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (57.4.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.46.3)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.15.0)\n","Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.26.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.1.0)\n","Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (14.0.1)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.14.1)\n","Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.5.3)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (4.2.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.3.0)\n","Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.2.0)\n","Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.21.6)\n","Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.0.0)\n","Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.17.3)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.6.3)\n","Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.0)\n","Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.2)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2) (0.37.1)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow->keras-rl2) (1.5.2)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.6)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.6.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.3.7)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.8.1)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.23.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.0.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.35.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.2.4)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.11.4)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.8.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2022.5.18.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.2.0)\n"]}],"source":["!pip install keras-rl2\n"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"6zqOEe4ZaFAo","executionInfo":{"status":"ok","timestamp":1654755554291,"user_tz":-330,"elapsed":11,"user":{"displayName":"Arti Gupta","userId":"17690262578366801034"}}},"outputs":[],"source":["from rl.agents import DQNAgent\n","from rl.policy import BoltzmannQPolicy\n","from rl.memory import SequentialMemory"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"KYacQiKzaFAo","executionInfo":{"status":"ok","timestamp":1654755554292,"user_tz":-330,"elapsed":10,"user":{"displayName":"Arti Gupta","userId":"17690262578366801034"}}},"outputs":[],"source":["def build_agent(model, actions):\n","    policy = BoltzmannQPolicy()\n","    memory = SequentialMemory(limit=12000, window_length=1)\n","    dqn = DQNAgent(model=model, memory=memory, policy=policy, \n","                  nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n","    return dqn"]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZTFd-qa7QLd6","executionInfo":{"status":"ok","timestamp":1654757123820,"user_tz":-330,"elapsed":88782,"user":{"displayName":"Arti Gupta","userId":"17690262578366801034"}},"outputId":"d8786f31-bae6-4350-b20e-e30279eb9ff0"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(Adam, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Training for 12000 steps ...\n","Interval 1 (0 steps performed)\n","\r    1/10000 [..............................] - ETA: 12:23 - reward: 0.8207"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  updates=self.state_updates,\n","/usr/local/lib/python3.7/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n","  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"]},{"output_type":"stream","name":"stdout","text":["10000/10000 [==============================] - 74s 7ms/step - reward: 0.3941\n","10 episodes - episode_reward: 394.107 [379.012, 403.440] - loss: 0.275 - mae: 12.551 - mean_q: 15.007\n","\n","Interval 2 (10000 steps performed)\n"," 2000/10000 [=====>........................] - ETA: 58s - reward: 0.4026done, took 88.401 seconds\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f29171c4f50>"]},"metadata":{},"execution_count":37}],"source":["#Run DRL based model\n","dqn = build_agent(model, actions)\n","dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n","dqn.fit(env_drl, nb_steps=12000, visualize=False, verbose=1)"]},{"cell_type":"code","source":["sscores = dqn.test(env_drl, nb_episodes = 100, visualize = False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xa4Rgt6C0QpI","executionInfo":{"status":"ok","timestamp":1654756918865,"user_tz":-330,"elapsed":63855,"user":{"displayName":"Arti Gupta","userId":"17690262578366801034"}},"outputId":"6dca4349-6a9e-4731-fd8e-9649298e60a6"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["Testing for 100 episodes ...\n","Episode 1: reward: -374.032, steps: 1000\n","Episode 2: reward: -374.007, steps: 1000\n","Episode 3: reward: -374.010, steps: 1000\n","Episode 4: reward: -374.018, steps: 1000\n","Episode 5: reward: -373.985, steps: 1000\n","Episode 6: reward: -374.005, steps: 1000\n","Episode 7: reward: -374.011, steps: 1000\n","Episode 8: reward: -373.982, steps: 1000\n","Episode 9: reward: -374.001, steps: 1000\n","Episode 10: reward: -374.009, steps: 1000\n","Episode 11: reward: -373.980, steps: 1000\n","Episode 12: reward: -374.024, steps: 1000\n","Episode 13: reward: -374.031, steps: 1000\n","Episode 14: reward: -373.990, steps: 1000\n","Episode 15: reward: -374.010, steps: 1000\n","Episode 16: reward: -374.013, steps: 1000\n","Episode 17: reward: -373.985, steps: 1000\n","Episode 18: reward: -374.002, steps: 1000\n","Episode 19: reward: -374.011, steps: 1000\n","Episode 20: reward: -373.981, steps: 1000\n","Episode 21: reward: -374.000, steps: 1000\n","Episode 22: reward: -374.008, steps: 1000\n","Episode 23: reward: -374.008, steps: 1000\n","Episode 24: reward: -374.023, steps: 1000\n","Episode 25: reward: -374.018, steps: 1000\n","Episode 26: reward: -373.990, steps: 1000\n","Episode 27: reward: -374.005, steps: 1000\n","Episode 28: reward: -374.013, steps: 1000\n","Episode 29: reward: -373.983, steps: 1000\n","Episode 30: reward: -374.002, steps: 1000\n","Episode 31: reward: -374.009, steps: 1000\n","Episode 32: reward: -373.981, steps: 1000\n","Episode 33: reward: -374.000, steps: 1000\n","Episode 34: reward: -374.032, steps: 1000\n","Episode 35: reward: -374.007, steps: 1000\n","Episode 36: reward: -374.010, steps: 1000\n","Episode 37: reward: -374.018, steps: 1000\n","Episode 38: reward: -373.985, steps: 1000\n","Episode 39: reward: -374.005, steps: 1000\n","Episode 40: reward: -374.011, steps: 1000\n","Episode 41: reward: -373.982, steps: 1000\n","Episode 42: reward: -374.001, steps: 1000\n","Episode 43: reward: -374.009, steps: 1000\n","Episode 44: reward: -373.980, steps: 1000\n","Episode 45: reward: -374.024, steps: 1000\n","Episode 46: reward: -374.031, steps: 1000\n","Episode 47: reward: -373.990, steps: 1000\n","Episode 48: reward: -374.010, steps: 1000\n","Episode 49: reward: -374.013, steps: 1000\n","Episode 50: reward: -373.985, steps: 1000\n","Episode 51: reward: -374.002, steps: 1000\n","Episode 52: reward: -374.011, steps: 1000\n","Episode 53: reward: -373.981, steps: 1000\n","Episode 54: reward: -374.000, steps: 1000\n","Episode 55: reward: -374.008, steps: 1000\n","Episode 56: reward: -374.008, steps: 1000\n","Episode 57: reward: -374.023, steps: 1000\n","Episode 58: reward: -374.018, steps: 1000\n","Episode 59: reward: -373.990, steps: 1000\n","Episode 60: reward: -374.005, steps: 1000\n","Episode 61: reward: -374.013, steps: 1000\n","Episode 62: reward: -373.983, steps: 1000\n","Episode 63: reward: -374.002, steps: 1000\n","Episode 64: reward: -374.009, steps: 1000\n","Episode 65: reward: -373.981, steps: 1000\n","Episode 66: reward: -374.000, steps: 1000\n","Episode 67: reward: -374.032, steps: 1000\n","Episode 68: reward: -374.007, steps: 1000\n","Episode 69: reward: -374.010, steps: 1000\n","Episode 70: reward: -374.018, steps: 1000\n","Episode 71: reward: -373.985, steps: 1000\n","Episode 72: reward: -374.005, steps: 1000\n","Episode 73: reward: -374.011, steps: 1000\n","Episode 74: reward: -373.982, steps: 1000\n","Episode 75: reward: -374.001, steps: 1000\n","Episode 76: reward: -374.009, steps: 1000\n","Episode 77: reward: -373.980, steps: 1000\n","Episode 78: reward: -374.024, steps: 1000\n","Episode 79: reward: -374.031, steps: 1000\n","Episode 80: reward: -373.990, steps: 1000\n","Episode 81: reward: -374.010, steps: 1000\n","Episode 82: reward: -374.013, steps: 1000\n","Episode 83: reward: -373.985, steps: 1000\n","Episode 84: reward: -374.002, steps: 1000\n","Episode 85: reward: -374.011, steps: 1000\n","Episode 86: reward: -373.981, steps: 1000\n","Episode 87: reward: -374.000, steps: 1000\n","Episode 88: reward: -374.008, steps: 1000\n","Episode 89: reward: -374.008, steps: 1000\n","Episode 90: reward: -374.023, steps: 1000\n","Episode 91: reward: -374.018, steps: 1000\n","Episode 92: reward: -373.990, steps: 1000\n","Episode 93: reward: -374.005, steps: 1000\n","Episode 94: reward: -374.013, steps: 1000\n","Episode 95: reward: -373.983, steps: 1000\n","Episode 96: reward: -374.002, steps: 1000\n","Episode 97: reward: -374.009, steps: 1000\n","Episode 98: reward: -373.981, steps: 1000\n","Episode 99: reward: -374.000, steps: 1000\n","Episode 100: reward: -374.032, steps: 1000\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1654671697686,"user":{"displayName":"Arti Gupta","userId":"17690262578366801034"},"user_tz":-330},"id":"1HjxIv6ki9sQ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a9537ba9-3308-44fd-fe09-63d64007b47e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["1776000"]},"metadata":{},"execution_count":39}],"source":["len(Network_energy_drl)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1654671697686,"user":{"displayName":"Arti Gupta","userId":"17690262578366801034"},"user_tz":-330},"id":"Pg_wbS-rIftW","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9e3f85c4-afa6-430d-a2b4-def2da459f74"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["222000"]},"metadata":{},"execution_count":40}],"source":["len(power_drl)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1654671697687,"user":{"displayName":"Arti Gupta","userId":"17690262578366801034"},"user_tz":-330},"id":"1RXaKU3zIwtR","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ddf26d80-4c35-4aea-e07f-7846c3a119f2"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["1000"]},"metadata":{},"execution_count":41}],"source":["len(drl) #packet send in drl"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HrE5_IycpiTH"},"outputs":[],"source":["my_array = np.array(Network_energy_drl)"]},{"cell_type":"code","source":["import pandas as pd"],"metadata":{"id":"grLi2bJTrbVU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ooltzmOyrj2H","executionInfo":{"status":"ok","timestamp":1654499383228,"user_tz":-330,"elapsed":20356,"user":{"displayName":"Arti Gupta","userId":"17690262578366801034"}},"outputId":"12b1c000-cbf6-4c77-ec28-dd97fe118cae"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["pd.DataFrame(my_array).to_csv('/content/drive/MyDrive/Colab Notebooks/B.csv')"],"metadata":{"id":"3bsfKl6UdGni"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d0rVmj23I6KZ"},"outputs":[],"source":["my_array1 = np.array(drl)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j4Bo6tAhRKtY"},"outputs":[],"source":["pd.DataFrame(my_array1).to_csv('/content/drive/MyDrive/Colab Notebooks/D.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0RwcMXWNpz_x"},"outputs":[],"source":["NTE=np.reshape(Network_energy_drl,(142030,8))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q5oKei-AqeIm"},"outputs":[],"source":["import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kTyMnvH3qJmE"},"outputs":[],"source":["df=pd.DataFrame(NTE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5jo__ClSvHnI"},"outputs":[],"source":["df.to_csv('/content/drive/MyDrive/Colab Notebooks/Engergy_drl.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7vPzjK9kJUbR"},"outputs":[],"source":["pd.DataFrame(my_array1).to_csv('/content/drive/MyDrive/Colab Notebooks/packetsend_drl.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VCwrdL2OVU-k","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1654500919483,"user_tz":-330,"elapsed":102969,"user":{"displayName":"Arti Gupta","userId":"17690262578366801034"}},"outputId":"793b6bb3-46fb-4ced-824e-f1487a26e584"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(Adam, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Training for 12000 steps ...\n","Interval 1 (0 steps performed)\n","\r    1/10000 [..............................] - ETA: 14:02 - reward: 0.5000"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  updates=self.state_updates,\n","/usr/local/lib/python3.7/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n","  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"]},{"output_type":"stream","name":"stdout","text":["10000/10000 [==============================] - 85s 9ms/step - reward: 0.3887\n","10 episodes - episode_reward: 388.692 [387.829, 390.826] - loss: 0.340 - mae: 12.694 - mean_q: 15.340\n","\n","Interval 2 (10000 steps performed)\n"," 1997/10000 [====>.........................] - ETA: 1:08 - reward: 0.3892done, took 102.365 seconds\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f3c8d41f690>"]},"metadata":{},"execution_count":73}],"source":["#Run Baseline Model\n","dqn = build_agent(model, actions)\n","dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n","dqn.fit(env_bsl, nb_steps=12000, visualize=False, verbose=1)"]},{"cell_type":"code","source":["sscores_bsl = dqn.test(env_bsl, nb_episodes = 100, visualize = False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"It-Z3RDN0jVO","executionInfo":{"status":"ok","timestamp":1654501060022,"user_tz":-330,"elapsed":67658,"user":{"displayName":"Arti Gupta","userId":"17690262578366801034"}},"outputId":"15753a3b-7c00-4746-9e44-6bdd71eff828"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Testing for 100 episodes ...\n","Episode 1: reward: 252.078, steps: 1000\n","Episode 2: reward: 252.078, steps: 1000\n","Episode 3: reward: 252.078, steps: 1000\n","Episode 4: reward: 252.078, steps: 1000\n","Episode 5: reward: 252.078, steps: 1000\n","Episode 6: reward: 252.078, steps: 1000\n","Episode 7: reward: 252.078, steps: 1000\n","Episode 8: reward: 252.078, steps: 1000\n","Episode 9: reward: 252.078, steps: 1000\n","Episode 10: reward: 252.078, steps: 1000\n","Episode 11: reward: 252.078, steps: 1000\n","Episode 12: reward: 252.078, steps: 1000\n","Episode 13: reward: 252.078, steps: 1000\n","Episode 14: reward: 252.078, steps: 1000\n","Episode 15: reward: 252.078, steps: 1000\n","Episode 16: reward: 252.078, steps: 1000\n","Episode 17: reward: 252.078, steps: 1000\n","Episode 18: reward: 252.078, steps: 1000\n","Episode 19: reward: 252.078, steps: 1000\n","Episode 20: reward: 252.078, steps: 1000\n","Episode 21: reward: 252.078, steps: 1000\n","Episode 22: reward: 252.078, steps: 1000\n","Episode 23: reward: 252.078, steps: 1000\n","Episode 24: reward: 252.078, steps: 1000\n","Episode 25: reward: 252.078, steps: 1000\n","Episode 26: reward: 252.078, steps: 1000\n","Episode 27: reward: 252.078, steps: 1000\n","Episode 28: reward: 252.078, steps: 1000\n","Episode 29: reward: 252.078, steps: 1000\n","Episode 30: reward: 252.078, steps: 1000\n","Episode 31: reward: 252.078, steps: 1000\n","Episode 32: reward: 252.078, steps: 1000\n","Episode 33: reward: 252.078, steps: 1000\n","Episode 34: reward: 252.078, steps: 1000\n","Episode 35: reward: 252.078, steps: 1000\n","Episode 36: reward: 252.078, steps: 1000\n","Episode 37: reward: 252.078, steps: 1000\n","Episode 38: reward: 252.078, steps: 1000\n","Episode 39: reward: 252.078, steps: 1000\n","Episode 40: reward: 252.078, steps: 1000\n","Episode 41: reward: 252.078, steps: 1000\n","Episode 42: reward: 252.078, steps: 1000\n","Episode 43: reward: 252.078, steps: 1000\n","Episode 44: reward: 252.078, steps: 1000\n","Episode 45: reward: 252.078, steps: 1000\n","Episode 46: reward: 252.078, steps: 1000\n","Episode 47: reward: 252.078, steps: 1000\n","Episode 48: reward: 252.078, steps: 1000\n","Episode 49: reward: 252.078, steps: 1000\n","Episode 50: reward: 252.078, steps: 1000\n","Episode 51: reward: 252.078, steps: 1000\n","Episode 52: reward: 252.078, steps: 1000\n","Episode 53: reward: 252.078, steps: 1000\n","Episode 54: reward: 252.078, steps: 1000\n","Episode 55: reward: 252.078, steps: 1000\n","Episode 56: reward: 252.078, steps: 1000\n","Episode 57: reward: 252.078, steps: 1000\n","Episode 58: reward: 252.078, steps: 1000\n","Episode 59: reward: 252.078, steps: 1000\n","Episode 60: reward: 252.078, steps: 1000\n","Episode 61: reward: 252.078, steps: 1000\n","Episode 62: reward: 252.078, steps: 1000\n","Episode 63: reward: 252.078, steps: 1000\n","Episode 64: reward: 252.078, steps: 1000\n","Episode 65: reward: 252.078, steps: 1000\n","Episode 66: reward: 252.078, steps: 1000\n","Episode 67: reward: 252.078, steps: 1000\n","Episode 68: reward: 252.078, steps: 1000\n","Episode 69: reward: 252.078, steps: 1000\n","Episode 70: reward: 252.078, steps: 1000\n","Episode 71: reward: 252.078, steps: 1000\n","Episode 72: reward: 252.078, steps: 1000\n","Episode 73: reward: 252.078, steps: 1000\n","Episode 74: reward: 252.078, steps: 1000\n","Episode 75: reward: 252.078, steps: 1000\n","Episode 76: reward: 252.078, steps: 1000\n","Episode 77: reward: 252.078, steps: 1000\n","Episode 78: reward: 252.078, steps: 1000\n","Episode 79: reward: 252.078, steps: 1000\n","Episode 80: reward: 252.078, steps: 1000\n","Episode 81: reward: 252.078, steps: 1000\n","Episode 82: reward: 252.078, steps: 1000\n","Episode 83: reward: 252.078, steps: 1000\n","Episode 84: reward: 252.078, steps: 1000\n","Episode 85: reward: 252.078, steps: 1000\n","Episode 86: reward: 252.078, steps: 1000\n","Episode 87: reward: 252.078, steps: 1000\n","Episode 88: reward: 252.078, steps: 1000\n","Episode 89: reward: 252.078, steps: 1000\n","Episode 90: reward: 252.078, steps: 1000\n","Episode 91: reward: 252.078, steps: 1000\n","Episode 92: reward: 252.078, steps: 1000\n","Episode 93: reward: 252.078, steps: 1000\n","Episode 94: reward: 252.078, steps: 1000\n","Episode 95: reward: 252.078, steps: 1000\n","Episode 96: reward: 252.078, steps: 1000\n","Episode 97: reward: 252.078, steps: 1000\n","Episode 98: reward: 252.078, steps: 1000\n","Episode 99: reward: 252.078, steps: 1000\n","Episode 100: reward: 252.078, steps: 1000\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d6bf-G0eA248"},"outputs":[],"source":["df.to_csv('/content/drive/MyDrive/Colab Notebooks/Engergy_baseline.csv')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QDzHGLOnBRV9"},"outputs":[],"source":["df.to_csv('/content/drive/MyDrive/Colab Notebooks/power_drl.csv')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mpc7s8G8zPAR"},"outputs":[],"source":["len(Network_energy_drl)\n","my_array = np.array(Network_energy_drl)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oLWkJweVaFAp"},"outputs":[],"source":["print(len(drl))\n","print(len(baseline))\n","print(len(baseline))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x0Y2tbw_aFAp"},"outputs":[],"source":["print(power_drl)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cn-0N6JxzzVx"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bWkvggS4z3PY"},"outputs":[],"source":["A = np.array(Network_energy_baseline)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gyew67jG4Wwj"},"outputs":[],"source":["B = np.array(baseline)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7gm6XaHR0Z7t"},"outputs":[],"source":["pd.DataFrame(A).to_csv('/content/drive/MyDrive/Colab Notebooks/Network_energy_baseline.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U6viDiYh4QDr"},"outputs":[],"source":["pd.DataFrame(B).to_csv('/content/drive/MyDrive/Colab Notebooks/[packet_baseline.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5Kszh2tKaFAq"},"outputs":[],"source":["#Taking every 1000th sample for plotting in DRL.\n","res = []\n","i = 0\n","\n","while(i < 18000):\n","    res.append(power_drl[i])\n","    i += 1000\n","res.append(power_drl[-1])    \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"01T86fI0aFAq","scrolled":true},"outputs":[],"source":["print(res)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PLR_CaeFBsIT"},"outputs":[],"source":["df.to_csv('/content/drive/MyDrive/Colab Notebooks/res.csv')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VLt5lB0EaFAq"},"outputs":[],"source":["#Taking every 1000th sample for plotting in Baseline.\n","\n","res1 = []\n","i = 0\n","\n","while(i < 12000):\n","    res1.append(power_baseline[i])\n","    i += 1000\n","    \n","res1.append(power_baseline[-1])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MkAY3JMeCUOq"},"outputs":[],"source":["print(res)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7ODssc4gB9lS"},"outputs":[],"source":["df.to_csv('/content/drive/MyDrive/Colab Notebooks/res1_baseline.csv')\n"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"RL_CODE_e3.ipynb","provenance":[{"file_id":"1S0wddkFQTB7tKZiglV7JFXDxF9uKrWH-","timestamp":1641729108206}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":0}