{"cells":[{"cell_type":"markdown","source":[""],"metadata":{"id":"wnwo8lwvocAp"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"1IufrcOqaFAY"},"outputs":[],"source":["from gym import Env\n","from gym.spaces import Discrete, Box\n","import numpy as np\n","import random"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nZVWf5VRaFAa"},"outputs":[],"source":["baseline = []  #packets sent in baseline\n","drl = []       #packets sent in DRL  \n","power_baseline = []\n","power_drl = []\n","Network_energy_drl=[]\n","Network_energy_baseline=[]"]},{"cell_type":"markdown","metadata":{"id":"R-_eNHEO7Iy2"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"NQL-0Oxz40pW"},"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ajra87t-aFAb"},"outputs":[],"source":["# import matplotlib.pyplot as plt\n","# import numpy as np\n","\n","# X = np.array([i for i in range(1000)])\n","# y = np.array(baseline_power)\n","# z = np.array(drl_power)\n","\n","\n","# plt.plot(X, y, color='r', label='Baseline model')\n","# plt.plot(X, z, color='g', label='DRL based model')\n","  \n","# # Naming the x-axis, y-axis and the whole graph\n","\n","# plt.xlabel(\"Episodes\")\n","# plt.ylabel(\"Power Available\")\n","# plt.title(\"Battery Drain w.r.to Baseline and DRL\")\n","  \n","# # Adding legend, which helps us recognize the curve according to it's color\n","# plt.legend()\n","# plt.xlim([0, 1200]) \n","# plt.ylim([0, 18000]) \n","# plt.rcParams[\"figure.figsize\"] = (14,7)\n","# # To load the display window\n","# plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8xOzRKBYaFAc"},"outputs":[],"source":["class Wireless_Env_Baseline(Env):\n","    def __init__(self):\n","        \n","        #Packet Generation Rates calculated from Auto-Regression Model\n","        self.pgr = [1, 1, 1, 1, 1, 1, 1, 1]\n","        \n","        #These are packet queues for each sensor\n","        self.pulse = []\n","        self.heart_rate = []\n","        self.temperature = []\n","        self.diabetes = []\n","        self.spo2 = []\n","        self.ibi = []\n","        self.eda = []\n","        self.acc = []\n","      \n","        \n","        #Total initial power\n","        self.power = 4        \n","        \n","        \n","        #Step count for each episode. It can be changed as per requirement.\n","        self.count = 1001\n","        \n","        #self.sensors = {0 : \"Pulse\", 1 : \"Heart Rate\", 2 : \"Temprature\", 3 : \"Diabetes\",  : \"SpO2\", 5 : \"EDA\", 6 : \"IBI\", 7 : \"ACC\"}\n","                \n","        # Actions we can take ---> we can select any of 8 sensors for packet transmission.\n","        self.action_space = Discrete(8)\n","        \n","        \n","        #initial timestamp (Optional---> I have used it for testing purpose.)\n","        self.time_stamp = 1\n","        \n","        #Set the network length(Will update the battery power,,, We can update here the minimum battery sensor)\n","        self.life = 1000        \n","        \n","        #Initial number of packets in queues\n","        self.state = 0        \n","        \n","        \n","        #This is observation space..will take any random value( Just for model shake...No use in our case.)\n","        self.observation_space = Box(low=np.array([0]), high=np.array([100]))\n","    \n","    \n","    #calculating reward based on our reward function.\n","    def get_reward(self, packet_gen_prob, self_queue, others_queue):\n","        #print(\"In Reward\")\n","        reward_ = ((len(self_queue) * packet_gen_prob) + 1) / (others_queue - len(self_queue) + 1) + .25\n","        return reward_\n","     \n","        \n","    #This is crucial function. It defines how the state is changing after each selection. Every update is happening here.    \n","    def state_change(self): \n","        \n","        #In baseline...adding 1 packet into each sensor queue as each sensor is sensing 1 packet per step.\n","        self.ibi.append(1)\n","        self.eda.append(1)        \n","        self.diabetes.append(1)        \n","        self.heart_rate.append(1)\n","        self.acc.append(1)       \n","        self.temperature.append(1)        \n","        self.pulse.append(1)       \n","        self.spo2.append(1)\n","            \n","    def step(self, action):       \n","        \n","        #for testing.\n","        self.count -= 1\n","        \n","        #Power equation used here. Can be changed as per requirement.\n","        self.power -= (16.31 * 10e-9 * 256 + 1.97 * 10e-9 * 100 * 256)\n","        \n","        #Testing\n","        #print(\"Selected sensor is --> \", action)\n","        \n","        #Changing the state.\n","        self.state_change()\n","        \n","        total_pending_packets = len(self.pulse) + len(self.heart_rate) + len(self.temperature) + len(self.diabetes) + len(self.spo2) + len(self.ibi) + len(self.eda) + len(self.acc) \n","        \n","        #print(\"total pending packets -----> \", total_pending_packets)\n","        \n","        \n","        #In this if else ladder, we are updating based on selected node.\n","        \n","        if(self.count > 0):\n","            baseline.append(total_pending_packets)\n","#            \n","        if(self.power > 0):\n","            power_baseline.append(self.power)\n","        if(self.power <= 0):\n","            power_baseline.append(0)\n","        Network_energy_baseline.append(self.power)\n","\n","\n","\n","        if action == 0:\n","            #print(\" Sensor Queue size is \", len(self.pulse))\n","            if(len(self.pulse) == 0):\n","                reward = 0\n","            else:\n","                \n","                reward = self.get_reward(self.pgr[0], self.pulse, total_pending_packets)\n","                if(len(self.pulse) != 0):\n","                    self.pulse.pop()\n","        \n","        elif action == 1:\n","            #print(\" Sensor Queue size is \", len(self.heart_rate))\n","            if(len(self.heart_rate) == 0):\n","                reward = 0\n","            else:\n","                \n","                reward = self.get_reward(self.pgr[1], self.heart_rate, total_pending_packets)\n","                if len(self.heart_rate) != 0:\n","                    self.heart_rate.pop()\n","        \n","        elif action == 2:\n","            #print(\" Sensor Queue size is \", len(self.temperature))\n","            if len(self.temperature) == 0:\n","                reward = 0\n","            else:\n","                \n","                reward = self.get_reward(self.pgr[2], self.temperature, total_pending_packets)        \n","                if len(self.temperature) != 0:\n","                    self.temperature.pop()\n","            \n","        elif action == 3:\n","            #print(\" Sensor Queue size is \", len(self.diabetes ) )\n","            if(len(self.diabetes) == 0):\n","                reward = 0\n","            else:\n","                \n","                reward = self.get_reward(self.pgr[3], self.diabetes, total_pending_packets)\n","                if len(self.diabetes) != 0:\n","                    self.diabetes.pop()\n","        \n","        elif action == 4:\n","            #print(\" Sensor Queue size is \", len(self.spo2))\n","            if(len(self.spo2) == 0):\n","                reward = 0\n","            else:\n","                \n","                reward = self.get_reward(self.pgr[4], self.spo2, total_pending_packets)        \n","                if len(self.spo2) != 0:\n","                    self.spo2.pop()\n","        elif action == 5:\n","            #print(\" Sensor Queue size is \",len(self.eda))\n","            if(len(self.eda) == 0):\n","                reward = 0\n","            else:\n","                \n","                reward = self.get_reward(self.pgr[5], self.eda, total_pending_packets)        \n","                if len(self.eda) != 0:\n","                    self.eda.pop()\n","        elif action == 6:\n","            #print(\" Sensor Queue size is \", len(self.ibi))\n","            if(len(self.ibi) == 0):\n","                reward = 0\n","            else:\n","                \n","                reward = self.get_reward(self.pgr[6], self.ibi, total_pending_packets)        \n","                if len(self.ibi) != 0:\n","                    self.ibi.pop()\n","        else:\n","            #print(\" Sensor Queue size is \",len(self.acc))\n","            if(len(self.acc) == 0):\n","                reward = 0\n","            else:\n","                \n","                reward = self.get_reward(self.pgr[7], self.acc, total_pending_packets)        \n","                if len(self.acc) != 0:\n","                    self.acc.pop()  \n","        \n","        \n","        #Reducing life by 1, here we can also use battery power. \n","        self.life -= 1    \n","        \n","       \n","        \n","        if self.life <= 0: \n","            done = True\n","        else:\n","            done = False   \n","        \n","        # Apply temperature noise\n","        #self.state += random.randint(-1,1)\n","        \n","        # Set placeholder for info\n","        info = {}\n","        \n","        # Return step information\n","        return self.state, reward, done, info\n","\n","    def render(self):\n","        #For Visualization Purpose\n","        pass\n","    \n","    def reset(self):\n","        # Reset shower temperature\n","        self.state = 0\n","        # Reset shower time\n","        self.life = 1000\n","        \n","        #Reset all packet queues\n","        #print('=---------p--------------',self.pulse)\n","        self.pulse = []\n","        self.heart_rate = []\n","        self.temperature = []\n","        self.diabetes = []\n","        self.spo2 = []\n","        self.ibi = []\n","        self.eda = []\n","        self.acc = []\n","        \n","        return self.state\n","    \n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ryb9w6_dRzPe"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"enx6kP9jaFAf"},"outputs":[],"source":["\n","#Here also, all parameters are same as Baseline Model apart from PGR.\n","class Wireless_Env_DRL(Env):\n","    def __init__(self):\n","        \n","        #Packet Generation Rates calculated from Auto-Regression Model\n","        #self.pgr = [0.09, 0.298, 0.053, 0.793, 0.03, 0.978, 0.953, 0.244]\n","        self.pgr = [11,3,20,1,30,1,1,4]\n","        self.pulse = []\n","        self.heart_rate = []\n","        self.temperature = []\n","        self.diabetes = []\n","        self.spo2 = []\n","        self.ibi = []\n","        self.eda = []\n","        self.acc = []\n","        \n","        self.power = [.5, .5, .5, .5, .5, .5, .5, .5]\n","        self.count = 1001\n","        #self.flag = 1\n","        #self.sensors = {0 : \"Pulse\", 1 : \"Heart Rate\", 2 : \"Temprature\", 3 : \"Diabetes\",  : \"SpO2\", 5 : \"EDA\", 6 : \"IBI\", 7 : \"ACC\"}\n","                \n","        # Actions we can take, down, stay, up\n","        self.action_space = Discrete(8)\n","        \n","        self.time_stamp = 1\n","        \n","        #Set the network length(Will update the battery power,,, We can update here the minimum battery sensor)\n","        self.life = 1000       \n","        \n","        #Initial number of packets in queues\n","        self.state = 0        \n","        \n","        self.observation_space = Box(low=np.array([0]), high=np.array([100]))\n","        \n","    def get_reward(self, packet_gen_prob, self_queue, others_queue):\n","        #print(\"In Reward\")\n","        reward_ = (((len(self_queue) * packet_gen_prob) + 1) / (others_queue - len(self_queue) + 1) + .25)\n","        return reward_\n","        \n","    def state_change(self): \n","        \n","        ts = self.time_stamp\n","        \n","        self.time_stamp += 1\n","        \n","        self.ibi.append(1)\n","        self.eda.append(1)        \n","        self.diabetes.append(1)        \n","        \n","        if ts % 3 == 0:\n","            self.heart_rate.append(1)\n","        \n","        if ts % 4 == 0:\n","            self.acc.append(1)\n","        \n","        if ts % 20 == 0:\n","            self.temperature.append(1)\n","            \n","        if ts % 11 == 0:\n","            self.pulse.append(1)            \n","            \n","        if ts % 30 == 0:\n","            self.spo2.append(1)\n","            \n","        \n","    \n","    def step(self, action):       \n","        \n","        self.count -= 1\n","        \n","        #Power function. But we will reduce power only when sensor is selected.\n","        p = (16.31 * 10e-9 * 256 + 1.97 * 10e-9 * 100 * 256)\n","        \n","        \n","        #print(\"Selected sensor is --> \", action)\n","        \n","        self.state_change()\n","        \n","        total_pending_packets = len(self.pulse) + len(self.heart_rate) + len(self.temperature) + len(self.diabetes) + len(self.spo2) + len(self.ibi) + len(self.eda) + len(self.acc) \n","        \n","        #print(\"total pending packets -----> \", total_pending_packets)\n","        \n","        if(self.count > 0):\n","            \n","            drl.append(total_pending_packets)\n","            \n","        if(sum(self.power) > 0):\n","            power_drl.append(sum(self.power))\n","        \n","        Network_energy_drl.extend(self.power)\n","        if action == 0:\n","            #print(\" Sensor Queue size is \", len(self.pulse))\n","            if(len(self.pulse) == 0):\n","                reward = 0\n","            else:\n","                if(self.power[0] - p > 0): #here I am reducing power if this sensor is selected. same for others as well.\n","                    self.power[0] -= p\n","                    #print(power)\n","                reward = self.get_reward(self.pgr[0], self.pulse, total_pending_packets)\n","                if(len(self.pulse) != 0):\n","                    self.pulse.pop()\n","        \n","        elif action == 1:\n","            #print(\" Sensor Queue size is \", len(self.heart_rate))\n","            if(len(self.heart_rate) == 0):\n","                reward = 0\n","            else:\n","                if(self.power[1] - p > 0):\n","                    self.power[1] -= p\n","                reward = self.get_reward(self.pgr[1], self.heart_rate, total_pending_packets)\n","                if len(self.heart_rate) != 0:\n","                    self.heart_rate.pop()\n","        \n","        elif action == 2:\n","            #print(\" Sensor Queue size is \", len(self.temperature))\n","            if len(self.temperature) == 0:\n","                reward = 0\n","            else:\n","                if(self.power[2] - p > 0):\n","                    self.power[2] -= p\n","                reward = self.get_reward(self.pgr[2], self.temperature, total_pending_packets)        \n","                if len(self.temperature) != 0:\n","                    self.temperature.pop()\n","            \n","        elif action == 3:\n","            #print(\" Sensor Queue size is \", len(self.diabetes ) )\n","            if(len(self.diabetes) == 0):\n","                reward = 0\n","            else:\n","                if(self.power[3] - p > 0):\n","                    self.power[3] -= p\n","                reward = self.get_reward(self.pgr[3], self.diabetes, total_pending_packets)\n","                if len(self.diabetes) != 0:\n","                    self.diabetes.pop()\n","        \n","        elif action == 4:\n","            #print(\" Sensor Queue size is \", len(self.spo2))\n","            if(len(self.spo2) == 0):\n","                reward = 0\n","            else:\n","                if(self.power[4] - p > 0):\n","                    self.power[4] -= p\n","                reward = self.get_reward(self.pgr[4], self.spo2, total_pending_packets)        \n","                if len(self.spo2) != 0:\n","                    self.spo2.pop()\n","        elif action == 5:\n","            #print(\" Sensor Queue size is \",len(self.eda))\n","            if(len(self.eda) == 0):\n","                reward = 0\n","            else:\n","                if(self.power[5] - p > 0):\n","                    self.power[5] -= p\n","                reward = self.get_reward(self.pgr[5], self.eda, total_pending_packets)        \n","                if len(self.eda) != 0:\n","                    self.eda.pop()\n","        elif action == 6:\n","            #print(\" Sensor Queue size is \", len(self.ibi))\n","            if(len(self.ibi) == 0):\n","                reward = 0\n","            else:\n","                if(self.power[6] - p > 0):\n","                    self.power[6] -= p\n","                reward = self.get_reward(self.pgr[6], self.ibi, total_pending_packets)        \n","                if len(self.ibi) != 0:\n","                    self.ibi.pop()\n","        else:\n","            #print(\" Sensor Queue size is \",len(self.acc))\n","            if(len(self.acc) == 0):\n","                reward = 0\n","            else:\n","                if(self.power[7] - p > 0):\n","                    self.power[7] -= p\n","                reward = self.get_reward(self.pgr[7], self.acc, total_pending_packets)        \n","                if len(self.acc) != 0:\n","                    self.acc.pop()  \n","        \n","        \n","        \n","        self.life -= 1    \n","            \n","        if self.life <= 0: \n","            done = True\n","        else:\n","            done = False   \n","        \n","        # Apply temperature noise\n","        #self.state += random.randint(-1,1)\n","        \n","        # Set placeholder for info\n","        info = {}\n","        \n","        # Return step information\n","        return self.state, reward, done, info\n","\n","    def render(self):\n","        #For Visualization Purpose\n","        pass\n","    \n","    #This step resets the parameters after each 1000 steps. For training purpose. can be updated for battery life as well.\n","    def reset(self):\n","        # Reset shower temperature\n","        self.state = 0\n","        # Reset shower time\n","        self.life = 1000\n","        \n","        #Reset all packet queues\n","        self.pulse = []\n","        self.heart_rate = []\n","        self.temperature = []\n","        self.diabetes = []\n","        self.spo2 = []\n","        self.ibi = []\n","        self.eda = []\n","        self.acc = []\n","        \n","        return self.state\n","    \n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":406,"status":"ok","timestamp":1656517773750,"user":{"displayName":"Arti Gupta","userId":"17690262578366801034"},"user_tz":-330},"id":"CQ3Ue_DkaFAh","outputId":"0921c537-4d58-4ceb-8ea4-a457a10e7614"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n","  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"]}],"source":["env_drl = Wireless_Env_DRL() "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1656517774181,"user":{"displayName":"Arti Gupta","userId":"17690262578366801034"},"user_tz":-330},"id":"3COZZ3iqaFAj","outputId":"f55f79a0-adad-4b68-8303-59258e9c2035"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n","  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"]}],"source":["env_bsl = Wireless_Env_Baseline()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1656517774779,"user":{"displayName":"Arti Gupta","userId":"17690262578366801034"},"user_tz":-330},"id":"hs64JwcJaFAk","outputId":"84b10a76-4195-4b31-829b-4c3d29d10f25"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([54.99886], dtype=float32)"]},"metadata":{},"execution_count":8}],"source":["env_bsl.observation_space.sample()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1656517775188,"user":{"displayName":"Arti Gupta","userId":"17690262578366801034"},"user_tz":-330},"id":"GX3gBMDNaFAk","outputId":"a6f29192-fe48-4887-8181-7864b4c1a279"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([74.37172], dtype=float32)"]},"metadata":{},"execution_count":9}],"source":["env_drl.observation_space.sample()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1654863037953,"user":{"displayName":"Arti Gupta","userId":"17690262578366801034"},"user_tz":-330},"id":"NRPPkw44x9ka","outputId":"60401d6d-1e44-4a87-a880-3371397fd85b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Episode:1 Reward:409.7216644339269\n","Episode:2 Reward:418.70846288381824\n","Episode:3 Reward:423.1878127631594\n","Episode:4 Reward:403.91990659691584\n","Episode:5 Reward:391.39052217042604\n","Episode:6 Reward:406.9583218947861\n","Episode:7 Reward:407.77663866832745\n","Episode:8 Reward:406.2701075847471\n","Episode:9 Reward:414.76501341623424\n","Episode:10 Reward:429.1301740874705\n"]}],"source":[" episodes = 10\n"," sco_drl=[]\n"," for episode in range(1, episodes+1):\n","     state = env_drl.reset()\n","     done = False\n","     score = 0 \n","    \n","     while not done:\n","         #env.render()\n","         action = env_drl.action_space.sample()\n","         n_state, reward, done, info = env_drl.step(action)\n","         score+=reward\n","     print('Episode:{} Reward:{}'.format(episode, score))\n","     sco_drl.append(score)\n"]},{"cell_type":"code","source":["sco_drl"],"metadata":{"id":"YLDztpAl2wqR"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BwMxvZ_qMCIG"},"outputs":[],"source":["import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"730ZILCgMHcF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1654864882548,"user_tz":-330,"elapsed":3166,"user":{"displayName":"Arti Gupta","userId":"17690262578366801034"}},"outputId":"aea323ba-a3a8-4a2e-c0aa-c39f5095a21d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-hc2h495MRU1"},"outputs":[],"source":["A = np.array(sco_drl)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l3AZev6CMbe9"},"outputs":[],"source":["pd.DataFrame(A).to_csv('/content/drive/MyDrive/Colab Notebooks/reward_drl_e2.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WT20BA4uaFAk"},"outputs":[],"source":["# episodes = 3\n","# sco=[]\n","# for episode in range(1, episodes+1):\n"," #  state = env_drl.reset()\n","  # done = False\n","   #score = 0 \n","   #while not done:\n","    # env.render()\n","   #  action = env_drl.action_space.sample()\n","    # n_state, reward, done, info = env_drl.step(action)\n","     #score+=reward\n","   #print('Episode:{} Reward:{}'.format(episode, score))\n","   #sco.append(score)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YVp54AW1y_q1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1654862526414,"user_tz":-330,"elapsed":12,"user":{"displayName":"Arti Gupta","userId":"17690262578366801034"}},"outputId":"5730eedb-8e65-4bba-b886-99ea1167cdee"},"outputs":[{"output_type":"stream","name":"stdout","text":["Episode:1 Reward:393.82088910764054\n","Episode:2 Reward:394.02189004939567\n","Episode:3 Reward:394.07571866207354\n","Episode:4 Reward:393.5266088989007\n","Episode:5 Reward:393.706387138371\n","Episode:6 Reward:393.71945395233297\n","Episode:7 Reward:393.8861080714335\n","Episode:8 Reward:393.88845736543084\n","Episode:9 Reward:393.695203499776\n","Episode:10 Reward:393.94767377308983\n"]}],"source":[" episodes = 10\n"," sco_bsl =[]\n"," for episode in range(1, episodes+1):\n","     state = env_bsl.reset()\n","     done = False\n","     score = 0 \n","    \n","     while not done:\n","         #env.render()\n","         action = env_bsl.action_space.sample()\n","         n_state, reward, done, info = env_bsl.step(action)\n","         score+=reward\n","     print('Episode:{} Reward:{}'.format(episode, score))\n","     sco_bsl.append(score)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EcliaFAuaFAl"},"outputs":[],"source":["import numpy as np\n","import tensorflow\n","import keras\n","from keras.models import Sequential\n","from keras.layers import Dense, Flatten\n","from tensorflow.keras.optimizers import Adam\n","model = keras.Sequential()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-XytEgM2aFAl"},"outputs":[],"source":["states = env_bsl.observation_space.shape\n","actions = env_bsl.action_space.n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-Om9y8S9aFAm"},"outputs":[],"source":["states = env_drl.observation_space.shape\n","actions = env_drl.action_space.n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1656517790859,"user":{"displayName":"Arti Gupta","userId":"17690262578366801034"},"user_tz":-330},"id":"rdhYDUwaaFAm","outputId":"88f97ccc-2318-4fa0-93f3-3ae0a908c446"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Box(0.0, 100.0, (1,), float32)"]},"metadata":{},"execution_count":13}],"source":["env_drl.observation_space"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":384,"status":"ok","timestamp":1656517842495,"user":{"displayName":"Arti Gupta","userId":"17690262578366801034"},"user_tz":-330},"id":"EJQqmh9p2DGW","outputId":"2f37c480-5c86-41d9-af8c-26778476f8b3"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Box(0.0, 100.0, (1,), float32)"]},"metadata":{},"execution_count":14}],"source":["env_bsl.observation_space"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1656517844171,"user":{"displayName":"Arti Gupta","userId":"17690262578366801034"},"user_tz":-330},"id":"1Di1yO10aFAm","outputId":"91924519-8ced-4a07-fe6e-1a709cf4ae8c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["8"]},"metadata":{},"execution_count":15}],"source":["actions\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ezRxfEpXoXmH"},"outputs":[],"source":["del model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tEsCsDPhaFAn"},"outputs":[],"source":["def build_model(states, actions):\n","    model = keras.Sequential()\n","    \n","    model.add(Dense(32, activation='relu', input_shape=states))\n","    model.add(Dense(64, activation='relu'))\n","    model.add(Dense(actions, activation='linear'))\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HQmgcM24aFAn"},"outputs":[],"source":["model = build_model(states, actions)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":591,"status":"ok","timestamp":1656517897535,"user":{"displayName":"Arti Gupta","userId":"17690262578366801034"},"user_tz":-330},"id":"I5ebzUI8aFAo","outputId":"d787bf4c-bb86-45d6-94c4-77d3be3aa04a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_3 (Dense)             (None, 32)                64        \n","                                                                 \n"," dense_4 (Dense)             (None, 64)                2112      \n","                                                                 \n"," dense_5 (Dense)             (None, 8)                 520       \n","                                                                 \n","=================================================================\n","Total params: 2,696\n","Trainable params: 2,696\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4300,"status":"ok","timestamp":1656517901832,"user":{"displayName":"Arti Gupta","userId":"17690262578366801034"},"user_tz":-330},"id":"DwpCt_yRbJSJ","outputId":"f05d5cec-43b2-4fe7-adf3-ecc948d1b995"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: keras-rl2 in /usr/local/lib/python3.7/dist-packages (1.0.5)\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from keras-rl2) (2.8.2+zzzcolab20220527125636)\n","Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (14.0.1)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.1.0)\n","Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.21.6)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.0)\n","Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.2)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.14.1)\n","Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n","Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.5.3)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.26.0)\n","Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.3.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (57.4.0)\n","Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.6.3)\n","Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.46.3)\n","Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.17.3)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.15.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (4.1.1)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.2.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2) (0.37.1)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow->keras-rl2) (1.5.2)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.8.1)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.6.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.3.7)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.6)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.0.1)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.23.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.35.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.2.4)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.11.4)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.8.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.10)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.2.0)\n"]}],"source":["!pip install keras-rl2\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6zqOEe4ZaFAo"},"outputs":[],"source":["from rl.agents import DQNAgent\n","from rl.policy import BoltzmannQPolicy\n","from rl.memory import SequentialMemory"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KYacQiKzaFAo"},"outputs":[],"source":["def build_agent(model, actions):\n","    policy = BoltzmannQPolicy()\n","    memory = SequentialMemory(limit=12000, window_length=1)\n","    dqn = DQNAgent(model=model, memory=memory, policy=policy, \n","                  nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n","    return dqn"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZTFd-qa7QLd6","outputId":"fe0804c2-b3bf-4327-c796-03b3d82dda1d","executionInfo":{"status":"ok","timestamp":1656518017622,"user_tz":-330,"elapsed":115796,"user":{"displayName":"Arti Gupta","userId":"17690262578366801034"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(Adam, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Training for 12000 steps ...\n","Interval 1 (0 steps performed)\n","\r    1/10000 [..............................] - ETA: 13:17 - reward: 0.9167"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  updates=self.state_updates,\n","/usr/local/lib/python3.7/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n","  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"]},{"output_type":"stream","name":"stdout","text":["10000/10000 [==============================] - 97s 10ms/step - reward: 0.4387\n","10 episodes - episode_reward: 438.710 [429.894, 466.742] - loss: 0.106 - mae: 3.934 - mean_q: 4.545\n","\n","Interval 2 (10000 steps performed)\n"," 1996/10000 [====>.........................] - ETA: 1:11 - reward: 0.4485done, took 115.265 seconds\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f36016e6910>"]},"metadata":{},"execution_count":31}],"source":["#Run DRL based model\n","dqn = build_agent(model, actions)\n","dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n","dqn.fit(env_drl, nb_steps=12000, visualize=False, verbose=1)"]},{"cell_type":"markdown","source":[""],"metadata":{"id":"WfpjR9dyhJz4"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xa4Rgt6C0QpI","outputId":"cf4e52e8-b7b0-42cc-9a49-bf6963757957","executionInfo":{"status":"ok","timestamp":1656518118090,"user_tz":-330,"elapsed":69639,"user":{"displayName":"Arti Gupta","userId":"17690262578366801034"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Testing for 100 episodes ...\n","Episode 1: reward: 255.145, steps: 1000\n","Episode 2: reward: 255.238, steps: 1000\n","Episode 3: reward: 255.077, steps: 1000\n","Episode 4: reward: 255.415, steps: 1000\n","Episode 5: reward: 255.336, steps: 1000\n","Episode 6: reward: 255.134, steps: 1000\n","Episode 7: reward: 255.455, steps: 1000\n","Episode 8: reward: 255.365, steps: 1000\n","Episode 9: reward: 255.156, steps: 1000\n","Episode 10: reward: 255.473, steps: 1000\n","Episode 11: reward: 255.380, steps: 1000\n","Episode 12: reward: 254.924, steps: 1000\n","Episode 13: reward: 255.315, steps: 1000\n","Episode 14: reward: 255.289, steps: 1000\n","Episode 15: reward: 255.107, steps: 1000\n","Episode 16: reward: 255.432, steps: 1000\n","Episode 17: reward: 255.348, steps: 1000\n","Episode 18: reward: 255.143, steps: 1000\n","Episode 19: reward: 255.462, steps: 1000\n","Episode 20: reward: 255.371, steps: 1000\n","Episode 21: reward: 255.161, steps: 1000\n","Episode 22: reward: 255.478, steps: 1000\n","Episode 23: reward: 255.068, steps: 1000\n","Episode 24: reward: 255.027, steps: 1000\n","Episode 25: reward: 255.384, steps: 1000\n","Episode 26: reward: 255.319, steps: 1000\n","Episode 27: reward: 255.123, steps: 1000\n","Episode 28: reward: 255.445, steps: 1000\n","Episode 29: reward: 255.357, steps: 1000\n","Episode 30: reward: 255.150, steps: 1000\n","Episode 31: reward: 255.469, steps: 1000\n","Episode 32: reward: 255.375, steps: 1000\n","Episode 33: reward: 255.165, steps: 1000\n","Episode 34: reward: 255.145, steps: 1000\n","Episode 35: reward: 255.238, steps: 1000\n","Episode 36: reward: 255.077, steps: 1000\n","Episode 37: reward: 255.415, steps: 1000\n","Episode 38: reward: 255.336, steps: 1000\n","Episode 39: reward: 255.134, steps: 1000\n","Episode 40: reward: 255.455, steps: 1000\n","Episode 41: reward: 255.365, steps: 1000\n","Episode 42: reward: 255.156, steps: 1000\n","Episode 43: reward: 255.473, steps: 1000\n","Episode 44: reward: 255.380, steps: 1000\n","Episode 45: reward: 254.924, steps: 1000\n","Episode 46: reward: 255.315, steps: 1000\n","Episode 47: reward: 255.289, steps: 1000\n","Episode 48: reward: 255.107, steps: 1000\n","Episode 49: reward: 255.432, steps: 1000\n","Episode 50: reward: 255.348, steps: 1000\n","Episode 51: reward: 255.143, steps: 1000\n","Episode 52: reward: 255.462, steps: 1000\n","Episode 53: reward: 255.371, steps: 1000\n","Episode 54: reward: 255.161, steps: 1000\n","Episode 55: reward: 255.478, steps: 1000\n","Episode 56: reward: 255.068, steps: 1000\n","Episode 57: reward: 255.027, steps: 1000\n","Episode 58: reward: 255.384, steps: 1000\n","Episode 59: reward: 255.319, steps: 1000\n","Episode 60: reward: 255.123, steps: 1000\n","Episode 61: reward: 255.445, steps: 1000\n","Episode 62: reward: 255.357, steps: 1000\n","Episode 63: reward: 255.150, steps: 1000\n","Episode 64: reward: 255.469, steps: 1000\n","Episode 65: reward: 255.375, steps: 1000\n","Episode 66: reward: 255.165, steps: 1000\n","Episode 67: reward: 255.145, steps: 1000\n","Episode 68: reward: 255.238, steps: 1000\n","Episode 69: reward: 255.077, steps: 1000\n","Episode 70: reward: 255.415, steps: 1000\n","Episode 71: reward: 255.336, steps: 1000\n","Episode 72: reward: 255.134, steps: 1000\n","Episode 73: reward: 255.455, steps: 1000\n","Episode 74: reward: 255.365, steps: 1000\n","Episode 75: reward: 255.156, steps: 1000\n","Episode 76: reward: 255.473, steps: 1000\n","Episode 77: reward: 255.380, steps: 1000\n","Episode 78: reward: 254.924, steps: 1000\n","Episode 79: reward: 255.315, steps: 1000\n","Episode 80: reward: 255.289, steps: 1000\n","Episode 81: reward: 255.107, steps: 1000\n","Episode 82: reward: 255.432, steps: 1000\n","Episode 83: reward: 255.348, steps: 1000\n","Episode 84: reward: 255.143, steps: 1000\n","Episode 85: reward: 255.462, steps: 1000\n","Episode 86: reward: 255.371, steps: 1000\n","Episode 87: reward: 255.161, steps: 1000\n","Episode 88: reward: 255.478, steps: 1000\n","Episode 89: reward: 255.068, steps: 1000\n","Episode 90: reward: 255.027, steps: 1000\n","Episode 91: reward: 255.384, steps: 1000\n","Episode 92: reward: 255.319, steps: 1000\n","Episode 93: reward: 255.123, steps: 1000\n","Episode 94: reward: 255.445, steps: 1000\n","Episode 95: reward: 255.357, steps: 1000\n","Episode 96: reward: 255.150, steps: 1000\n","Episode 97: reward: 255.469, steps: 1000\n","Episode 98: reward: 255.375, steps: 1000\n","Episode 99: reward: 255.165, steps: 1000\n","Episode 100: reward: 255.145, steps: 1000\n"]}],"source":["sscores = dqn.test(env_drl, nb_episodes = 100, visualize = False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":700,"status":"ok","timestamp":1654865128957,"user":{"displayName":"Arti Gupta","userId":"17690262578366801034"},"user_tz":-330},"id":"1HjxIv6ki9sQ","outputId":"767dd653-9c95-405f-8303-f0a6767861c5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["96000"]},"metadata":{},"execution_count":85}],"source":["len(Network_energy_drl)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1654865133425,"user":{"displayName":"Arti Gupta","userId":"17690262578366801034"},"user_tz":-330},"id":"Pg_wbS-rIftW","outputId":"a7c3c9b0-4aa0-4867-c586-986573caf750"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["12000"]},"metadata":{},"execution_count":86}],"source":["len(power_drl)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":553,"status":"ok","timestamp":1654865136868,"user":{"displayName":"Arti Gupta","userId":"17690262578366801034"},"user_tz":-330},"id":"1RXaKU3zIwtR","outputId":"fa1e736f-9d9e-488e-f3d4-23bffc4e2bcc"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["1000"]},"metadata":{},"execution_count":87}],"source":["len(drl) #packet send in drl"]},{"cell_type":"code","source":["len(power_drl)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rUs4LxMHq1rz","executionInfo":{"status":"ok","timestamp":1654865496573,"user_tz":-330,"elapsed":597,"user":{"displayName":"Arti Gupta","userId":"17690262578366801034"}},"outputId":"47305015-c6f1-46b6-bc6e-17a096af1ab0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["12000"]},"metadata":{},"execution_count":95}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"grLi2bJTrbVU"},"outputs":[],"source":["import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20356,"status":"ok","timestamp":1654499383228,"user":{"displayName":"Arti Gupta","userId":"17690262578366801034"},"user_tz":-330},"id":"ooltzmOyrj2H","outputId":"12b1c000-cbf6-4c77-ec28-dd97fe118cae"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d0rVmj23I6KZ"},"outputs":[],"source":["my_array1 = np.array(drl)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j4Bo6tAhRKtY"},"outputs":[],"source":["pd.DataFrame(my_array1).to_csv('/content/drive/MyDrive/Colab Notebooks/packet_e2.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0RwcMXWNpz_x"},"outputs":[],"source":["NTE=np.reshape(Network_energy_drl,(12000,8))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q5oKei-AqeIm"},"outputs":[],"source":["import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kTyMnvH3qJmE"},"outputs":[],"source":["df=pd.DataFrame(NTE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5jo__ClSvHnI"},"outputs":[],"source":["df.to_csv('/content/drive/MyDrive/Colab Notebooks/Engergy_e2.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7vPzjK9kJUbR"},"outputs":[],"source":["pd.DataFrame(my_array1).to_csv('/content/drive/MyDrive/Colab Notebooks/packet_e2.csv')"]},{"cell_type":"code","source":["arti1 = np.array(power_drl)"],"metadata":{"id":"ElzLcMHaqVpK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pd.DataFrame(arti1).to_csv('/content/drive/MyDrive/Colab Notebooks/total_E_e2.csv')"],"metadata":{"id":"UDV07E-qq-By"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":103809,"status":"ok","timestamp":1656518221881,"user":{"displayName":"Arti Gupta","userId":"17690262578366801034"},"user_tz":-330},"id":"VCwrdL2OVU-k","outputId":"42352460-e7c9-4428-f759-99e5d0d97640"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(Adam, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Training for 12000 steps ...\n","Interval 1 (0 steps performed)\n","\r    1/10000 [..............................] - ETA: 14:41 - reward: 0.5000"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  updates=self.state_updates,\n","/usr/local/lib/python3.7/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n","  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"]},{"output_type":"stream","name":"stdout","text":["10000/10000 [==============================] - 86s 9ms/step - reward: 0.3932\n","10 episodes - episode_reward: 393.217 [392.782, 393.828] - loss: 0.173 - mae: 12.893 - mean_q: 14.884\n","\n","Interval 2 (10000 steps performed)\n"," 2000/10000 [=====>........................] - ETA: 1:09 - reward: 0.3934done, took 103.561 seconds\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f3600c89e90>"]},"metadata":{},"execution_count":33}],"source":["#Run Baseline Model\n","dqn = build_agent(model, actions)\n","dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n","dqn.fit(env_bsl, nb_steps=12000, visualize=False, verbose=1)"]},{"cell_type":"code","source":[""],"metadata":{"id":"_Ga9LKCDqdop"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":67658,"status":"ok","timestamp":1654501060022,"user":{"displayName":"Arti Gupta","userId":"17690262578366801034"},"user_tz":-330},"id":"It-Z3RDN0jVO","outputId":"15753a3b-7c00-4746-9e44-6bdd71eff828"},"outputs":[{"name":"stdout","output_type":"stream","text":["Testing for 100 episodes ...\n","Episode 1: reward: 252.078, steps: 1000\n","Episode 2: reward: 252.078, steps: 1000\n","Episode 3: reward: 252.078, steps: 1000\n","Episode 4: reward: 252.078, steps: 1000\n","Episode 5: reward: 252.078, steps: 1000\n","Episode 6: reward: 252.078, steps: 1000\n","Episode 7: reward: 252.078, steps: 1000\n","Episode 8: reward: 252.078, steps: 1000\n","Episode 9: reward: 252.078, steps: 1000\n","Episode 10: reward: 252.078, steps: 1000\n","Episode 11: reward: 252.078, steps: 1000\n","Episode 12: reward: 252.078, steps: 1000\n","Episode 13: reward: 252.078, steps: 1000\n","Episode 14: reward: 252.078, steps: 1000\n","Episode 15: reward: 252.078, steps: 1000\n","Episode 16: reward: 252.078, steps: 1000\n","Episode 17: reward: 252.078, steps: 1000\n","Episode 18: reward: 252.078, steps: 1000\n","Episode 19: reward: 252.078, steps: 1000\n","Episode 20: reward: 252.078, steps: 1000\n","Episode 21: reward: 252.078, steps: 1000\n","Episode 22: reward: 252.078, steps: 1000\n","Episode 23: reward: 252.078, steps: 1000\n","Episode 24: reward: 252.078, steps: 1000\n","Episode 25: reward: 252.078, steps: 1000\n","Episode 26: reward: 252.078, steps: 1000\n","Episode 27: reward: 252.078, steps: 1000\n","Episode 28: reward: 252.078, steps: 1000\n","Episode 29: reward: 252.078, steps: 1000\n","Episode 30: reward: 252.078, steps: 1000\n","Episode 31: reward: 252.078, steps: 1000\n","Episode 32: reward: 252.078, steps: 1000\n","Episode 33: reward: 252.078, steps: 1000\n","Episode 34: reward: 252.078, steps: 1000\n","Episode 35: reward: 252.078, steps: 1000\n","Episode 36: reward: 252.078, steps: 1000\n","Episode 37: reward: 252.078, steps: 1000\n","Episode 38: reward: 252.078, steps: 1000\n","Episode 39: reward: 252.078, steps: 1000\n","Episode 40: reward: 252.078, steps: 1000\n","Episode 41: reward: 252.078, steps: 1000\n","Episode 42: reward: 252.078, steps: 1000\n","Episode 43: reward: 252.078, steps: 1000\n","Episode 44: reward: 252.078, steps: 1000\n","Episode 45: reward: 252.078, steps: 1000\n","Episode 46: reward: 252.078, steps: 1000\n","Episode 47: reward: 252.078, steps: 1000\n","Episode 48: reward: 252.078, steps: 1000\n","Episode 49: reward: 252.078, steps: 1000\n","Episode 50: reward: 252.078, steps: 1000\n","Episode 51: reward: 252.078, steps: 1000\n","Episode 52: reward: 252.078, steps: 1000\n","Episode 53: reward: 252.078, steps: 1000\n","Episode 54: reward: 252.078, steps: 1000\n","Episode 55: reward: 252.078, steps: 1000\n","Episode 56: reward: 252.078, steps: 1000\n","Episode 57: reward: 252.078, steps: 1000\n","Episode 58: reward: 252.078, steps: 1000\n","Episode 59: reward: 252.078, steps: 1000\n","Episode 60: reward: 252.078, steps: 1000\n","Episode 61: reward: 252.078, steps: 1000\n","Episode 62: reward: 252.078, steps: 1000\n","Episode 63: reward: 252.078, steps: 1000\n","Episode 64: reward: 252.078, steps: 1000\n","Episode 65: reward: 252.078, steps: 1000\n","Episode 66: reward: 252.078, steps: 1000\n","Episode 67: reward: 252.078, steps: 1000\n","Episode 68: reward: 252.078, steps: 1000\n","Episode 69: reward: 252.078, steps: 1000\n","Episode 70: reward: 252.078, steps: 1000\n","Episode 71: reward: 252.078, steps: 1000\n","Episode 72: reward: 252.078, steps: 1000\n","Episode 73: reward: 252.078, steps: 1000\n","Episode 74: reward: 252.078, steps: 1000\n","Episode 75: reward: 252.078, steps: 1000\n","Episode 76: reward: 252.078, steps: 1000\n","Episode 77: reward: 252.078, steps: 1000\n","Episode 78: reward: 252.078, steps: 1000\n","Episode 79: reward: 252.078, steps: 1000\n","Episode 80: reward: 252.078, steps: 1000\n","Episode 81: reward: 252.078, steps: 1000\n","Episode 82: reward: 252.078, steps: 1000\n","Episode 83: reward: 252.078, steps: 1000\n","Episode 84: reward: 252.078, steps: 1000\n","Episode 85: reward: 252.078, steps: 1000\n","Episode 86: reward: 252.078, steps: 1000\n","Episode 87: reward: 252.078, steps: 1000\n","Episode 88: reward: 252.078, steps: 1000\n","Episode 89: reward: 252.078, steps: 1000\n","Episode 90: reward: 252.078, steps: 1000\n","Episode 91: reward: 252.078, steps: 1000\n","Episode 92: reward: 252.078, steps: 1000\n","Episode 93: reward: 252.078, steps: 1000\n","Episode 94: reward: 252.078, steps: 1000\n","Episode 95: reward: 252.078, steps: 1000\n","Episode 96: reward: 252.078, steps: 1000\n","Episode 97: reward: 252.078, steps: 1000\n","Episode 98: reward: 252.078, steps: 1000\n","Episode 99: reward: 252.078, steps: 1000\n","Episode 100: reward: 252.078, steps: 1000\n"]}],"source":["#sscores_bsl = dqn.test(env_bsl, nb_episodes = 100, visualize = False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d6bf-G0eA248"},"outputs":[],"source":["df.to_csv('/content/drive/MyDrive/Colab Notebooks/Engergy_baseline.csv')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QDzHGLOnBRV9"},"outputs":[],"source":["df.to_csv('/content/drive/MyDrive/Colab Notebooks/power_drl.csv')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mpc7s8G8zPAR"},"outputs":[],"source":["len(Network_energy_drl)\n","my_array = np.array(Network_energy_drl)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oLWkJweVaFAp"},"outputs":[],"source":["print(len(drl))\n","print(len(baseline))\n","print(len(baseline))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x0Y2tbw_aFAp"},"outputs":[],"source":["print(power_drl)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cn-0N6JxzzVx"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bWkvggS4z3PY"},"outputs":[],"source":["A = np.array(Network_energy_baseline)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gyew67jG4Wwj"},"outputs":[],"source":["B = np.array(baseline)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7gm6XaHR0Z7t"},"outputs":[],"source":["pd.DataFrame(A).to_csv('/content/drive/MyDrive/Colab Notebooks/Network_energy_baseline.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U6viDiYh4QDr"},"outputs":[],"source":["pd.DataFrame(B).to_csv('/content/drive/MyDrive/Colab Notebooks/[packet_baseline.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5Kszh2tKaFAq"},"outputs":[],"source":["#Taking every 1000th sample for plotting in DRL.\n","res = []\n","i = 0\n","\n","while(i < 18000):\n","    res.append(power_drl[i])\n","    i += 1000\n","res.append(power_drl[-1])    \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"01T86fI0aFAq","scrolled":true},"outputs":[],"source":["print(res)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PLR_CaeFBsIT"},"outputs":[],"source":["df.to_csv('/content/drive/MyDrive/Colab Notebooks/res.csv')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VLt5lB0EaFAq"},"outputs":[],"source":["#Taking every 1000th sample for plotting in Baseline.\n","\n","res1 = []\n","i = 0\n","\n","while(i < 12000):\n","    res1.append(power_baseline[i])\n","    i += 1000\n","    \n","res1.append(power_baseline[-1])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MkAY3JMeCUOq"},"outputs":[],"source":["print(res)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7ODssc4gB9lS"},"outputs":[],"source":["df.to_csv('/content/drive/MyDrive/Colab Notebooks/res1_baseline.csv')\n"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"RL_CODE_e2.ipynb","provenance":[{"file_id":"1S0wddkFQTB7tKZiglV7JFXDxF9uKrWH-","timestamp":1641729108206}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":0}